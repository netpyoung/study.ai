# 머신 러닝(기계학습)


- 기계 학습 machine learning
  - 학습종류
    - 지도 학습(Supervised Learning): 레이블이 지정된 데이터를 사용하여
    - 비지도 학습(Unsupervised Learning): 레이블이 없는 데이터를 사용하
    - 강화 학습(Reinforcement Learning): 모델이 환경과 상호 작용하며 최적의 결정이나 행동을 학습
  - 분류, 회귀, 클러스터링, 차원 축소 등의 다양한 기계 학습 알고리즘 이해하기.



- https://cafe.naver.com/lisper/2560

- Machine Learning: The Art and Science of Algorithms that Make Sense of Data  (Peter Flach)
ML 시작하기에 최적의 책입니다. Tensorflow 등 요즘 유행하는 실용 지식은 없습니다!! 대신 50년에 걸친 인공지능 발전의 결과물을 체계적으로 정리한 책입니다. 이론적으로 접근하는 편이라..읽기 매우 어렵습니다. 중간중간 논문을 찾아 봐야하는 경우도 많습니다. 하지만..이책을 완독하면 현대 ML에 대한 기본이해는 꽉잡고 들어가는 겁니다.

쑥~읽을수 있는 책이 아닙니다. 하루 한장 읽기도 버겁내요. 하지만 보람 있습니다.


- No bullshit guide to math and physics (Ivan Savov)
ML은 확률통계, 이산수학, 미적분 에 대한 이해가 없으면 접근이 어렵습니다. 이를 위해 수학적 기초를 프로그래머(라고 쓰고 수포자라고 읽는다) 기준으로 쓴 책입니다. 머리에 쏙쏙 들어옵니다. 완독을 강추 합니다.



| key component |                                                      |
| ------------- | ---------------------------------------------------- |
| data          |                                                      |
| model         | AlexNet, GoogLeNet, LSTM, Deep AutoEncoders, GAN ... |
| loss function | 모델을 어떻게 학습할지                               |
| optimizer     | 최적화                                               |

- loss function
  - regression task - 회귀
  - classification task - 분류
  - probabilistic task - 확률

https://bbongcol.github.io/deep-learning-bookmarks/
https://github.com/teddylee777/machine-learning

| year |                          |                                                                               |
| ---- | ------------------------ | ----------------------------------------------------------------------------- |
| 2020 | Self Supervised Learning |                                                                               |
| 2019 | BIG language Models      |                                                                               |
| 2018 | BERT                     | Bidirectional Encoder Representations from Transformers                       |
| 2017 | **Transformer**          | Attention Is All You Need                                                     |
| 2015 | ResNet                   | Residual Networks // 네트워크를 깊게 쌓으면서 테스트에도 성능이 나오기 시작함 |
| 2015 | GAN                      | Generative Adversarial Network                                                |
| 2014 | Adam Optimizer           |                                                                               |
| 2014 | Encoder/Decoder          | NMT(뉴럴 머신 트렌스레이션) 문제를 풀기위한                                   |
| 2013 | DQN                      | deepmind -  강화 학습 가능한 심층 신경망을 이용                               |
| 2012 | AlexNet                  | CNN, 역사적으로 딥러닝 성능을 입증                                            |

- https://dennybritz.com/posts/deep-learning-ideas-that-stood-the-test-of-time/


